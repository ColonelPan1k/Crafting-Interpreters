* Setting up the Lexer/Scanner

--==> As a challenge, implement this lexer class in python and see if it can work on a
      theoretical language.  Play around and try to define something that might not
      be exactly "C-like." This doesn't have to become an actual language, just
      copy/write out a simple lexer in Python for some language which doesn't have
      the same specifications as Lox.

--==> Try writing out the "blueprint" for how this would work in C.  No reason to do it
      all now, since part 2 of the book will be this exact thing in C, but try to port it
      over while also dealing with some of the difficulties of C.


=> Basic program REPL/Interface just spits out the tokenized version of whatever is
   passed into it.

=> The lexer should take in some string, like "var language = "lox";" and return something
   like "IDENTIFIER var IDENTIFIER langauge EQUAL STRING "lox"" or something like that.

=> This is done in the book through one large enum that covers all the possible valid symbols and
   keywords, such as IDENTIFIER, STRING, NUMBER, EQUAL, GREATER_EQUAL, FUN, FOR, IF, NIL, etc...

=> This is then fed into some kind of class or struct which holds all the different information
   about a given token, such as the type (NUMBER, STRING, EQUAL, GREATER_EQUAL, FUN, etc), the
   lexeme (the string itself "var", "language", etc.), some literal, and the line number that
   it was found on.

=> The core of a scanner (what he calls a lexer) is just a loop that spits out some lexeme.
   It repeats that for the entire document until it hits an EOF

= = = > Extra theory: "The rules that determine how a particular language groups characters into
        lexemes are called its lexical grammer.  In Lox, the rules of that grammar are simple
        enough for the language to be classified as a regular language"

        => A "regular language" is a formal language which can be defined by a regular expression.

        => Look to the /Dragon Book/ for more of the theory behind lexers and compilers, etc. This
        books is only concerned with making them, rather than explaining the theory behind them.
          => For more theory, look up: Chomsky Hierarchy, regular languages, and finite-state machines.

** The Lexer/Scanner
The lexer/scanner is just some data structure that holds/consumes a source string, and returns
a list of tokens from that string.

=> The "TokenType" data type used in this is the enum.  Something like "typedef enum TokenType" in C.
   => this is paired with some function addToken(TokenType type) which takes in a TokenType object and
   adds it to some list of tokens.
   => In C, the token list, is probably a linked list which holds objects of type Token.


=> The lexer datatype holds a source string (the thing being read in), a list of tokens (the output)
   and three variables, the start, current, and line number.
   => Start and current keep track of where we are in a string.  "start" keeps track of the start of
   a given lexeme, and "current" is the current positon in a given lexeme.


=> There are also some methods that help us parse out tokens from a string.

=> scanToken() is a large switch/case function which moves increments the current and moves through
   the source string and gives us the token for current - 1.
   => This function is what actually converts the token from a random character/string to something
   we can use in our program
