* Setting up the Lexer/Scanner

--==> As a challenge, implement this lexer class in python and see if it can work on a
      theoretical language.  Play around and try to define something that might not
      be exactly "C-like." This doesn't have to become an actual language, just
      copy/write out a simple lexer in Python for some language which doesn't have
      the same specifications as Lox.

--==> Try writing out the "blueprint" for how this would work in C.  No reason to do it
      all now, since part 2 of the book will be this exact thing in C, but try to port it
      over while also dealing with some of the difficulties of C.


=> Basic program REPL/Interface just spits out the tokenized version of whatever is
   passed into it.

=> The lexer should take in some string, like "var language = "lox";" and return something
   like "IDENTIFIER var IDENTIFIER langauge EQUAL STRING "lox"" or something like that.

=> This is done in the book through one large enum that covers all the possible valid symbols and
   keywords, such as IDENTIFIER, STRING, NUMBER, EQUAL, GREATER_EQUAL, FUN, FOR, IF, NIL, etc...

=> This is then fed into some kind of class or struct which holds all the different information
   about a given token, such as the type (NUMBER, STRING, EQUAL, GREATER_EQUAL, FUN, etc), the
   lexeme (the string itself "var", "language", etc.), some literal, and the line number that
   it was found on.

=> The core of a scanner (what he calls a lexer) is just a loop that spits out some lexeme.
   It repeats that for the entire document until it hits an EOF

= = = > Extra theory: "The rules that determine how a particular language groups characters into
        lexemes are called its lexical grammer.  In Lox, the rules of that grammar are simple
        enough for the language to be classified as a regular language"

        => A "regular language" is a formal language which can be defined by a regular expression.

        => Look to the /Dragon Book/ for more of the theory behind lexers and compilers, etc. This
        books is only concerned with making them, rather than explaining the theory behind them.
          => For more theory, look up: Chomsky Hierarchy, regular languages, and finite-state machines.

** The Lexer/Scanner
The lexer/scanner is just some data structure that holds/consumes a source string, and returns
a list of tokens from that string.

=> The "TokenType" data type used in this is the enum.  Something like "typedef enum TokenType" in C.
   => this is paired with some function addToken(TokenType type) which takes in a TokenType object and
   adds it to some list of tokens.
   => In C, the token list, is probably a linked list which holds objects of type Token.


=> The lexer datatype holds a source string (the thing being read in), a list of tokens (the output)
   and three variables, the start, current, and line number.
   => Start and current keep track of where we are in a string.  "start" keeps track of the start of
   a given lexeme, and "current" is the current positon in a given lexeme.


=> There are also some methods that help us parse out tokens from a string.

=> scanToken() is a large switch/case function which moves increments the current and moves through
   the source string and gives us the token for current - 1.
   => This function is what actually converts the token from a random character/string to something
   we can use in our program
* Representing Code (Grammars and ASTs)
=> Two types of grammars, lexical and syntactic.

Lexical grammar (regular language):
  => The "alphabet" is made up of characters
  => A "string" is a lexeme or token
  => It is implemented through a lexer/scanner

Syntactic grammar (Context-free grammar):
  => The "alphabet" is made up of tokens
  => A "string" is an expression
  => It is implemented through a parser

Example:

_Lexical grammar_ defines how an incoming string should be tokenized. It's responsible for turning a string like:
"let x = 10;" into : LET "let", VAR "x", EQ "=", NUM "10"

_Syntactic grammar_ defines the valid and invalid arrangements of keywords.
These define what a valid expression is in a given language. This is the same
as writing out a grammar for a language like English (albeit not as complex).



The syntactic grammar is made up of two kinds of nodes: Terminal and nonterminal nodes.
=> A terminal node is any node which is a base/literal value in the language.
  => An expression parser uses NUMBER as a terminal value since a number is just
  itself with no other possible operations.  10 will always return 10

=> A nonterminal node is a node which references another rule in the grammar.
  => This would be operators in an expression parser.  An operator (PLUS, MINUS, STAR, SLASH)
  take in two terminal nodes, does something to them and returns a terminal node.


Computer languages use BNF (Backus-Nauer Form) to describe computer language grammar.

=> Each rule is a name, followed by an arrow (->), followed by a sequence of symbols
   and finally ending with a semicolon.  Terminals are quoted strings and nonterminals
   are lowercase words.

   => i.e. some keyword that does something is a quoted string whereas a variable that could
   be anything is a lowercase word

// this feels a little bit like paired down Haskell type statements
-- Simple grammar for an expression --
Expression -> number "plus" number;
Expression -> number "minus" number;
Expression -> number "times" number;
Expression -> number "divided by" number;

number -> [0-9];

--- A breakfast example grammar from the book ---

breakfast -> protein "with" breakfast "on the side";
breakfast -> protein;
breakfast -> bread;

protein -> crispiness "crispy" "bacon";
protein -> "sausage";
protein -> cooked "eggs";

crispiness -> "really"
crispiness -> "really" crispiness

cooked -> "scrambled";
cooked -> "poached";
cooked -> "fried";

bread -> "toast";
bread -> "biscuits";
bread -> "English muffin"

- Notice that each keyword can be followed to a resulting terminal node.

Generating random, valid, "breakfasts" are easy, you just have to follow each
resultant node to some number of terminal points, eg:

breakfast -> protein "with" breakfast "on the side";
  protein -> "sausage";
breakfast => "sausage" "with" breakfast "on the side"
    breakfast -> bread
        bread -> "toast"

Generates => "sausage" "with" "toast" "on the side"


=> We can extend and simplify our grammar using some operators, |, (), *, ?, +

=> Instead of
protein -> crispiness "crispy" "bacon";
protein -> "sausage";
protein -> cooked "eggs";

we could just write:
protein -> crispiness "crispy" "bacon" | "sausage" | cooked "eggs"

using the | operator

=> () allows for grouping
protein -> ("scrambled" | "poached" | "fried") "eggs"

=> * allows for better and easier recursion
crispiness -> "really" "really"*;

=> postfix +:
crispiness -> "really"+
allows for recusion with at least one of a given element

=> ? an optional production that can appear zero or one time:
breakfast -> protein ("with" breakfast "on the side")?;


-----------
// This is starting to look a lot closer to Haskell

Using these operators, we can condense the grammar down into these three
more simple rules

breakfast -> protein ("with" breakfast "on the side")
          | bread;

protein   -> "really"+ "crispy" "bacon"
          | "sausage"
          | ("scrambled" | "poached" | "fried") "eggs";

bread -> "toast" | "biscuits" | "English muffin";

--------------
A paired down version of Lox's syntactic grammar, just to get things up and running

Grammar for lox expressions:

expressions -> literal
            | unary
            | binary
            | grouping ;

literal     -> NUMBER | STRING | "true" | "false" | "nil";

grouping    -> "(" expression ")";

unary       -> ("-" | "!") expression;
binary      -> expression operator expression;
operator    -> "==" | "! =" | "<" | "< =" | ">" "> ="
            | "+" | "-" | "*" | "/"
